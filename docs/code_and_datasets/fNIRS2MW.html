<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The Tufts fNIRS to Mental Workload Dataset | Tufts HCI Lab</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="Tufts Human-Computer Interaction Laboratory">
    
    <link rel="preload" href="/assets/css/0.styles.3208571a.css" as="style"><link rel="preload" href="/assets/js/app.c05da18d.js" as="script"><link rel="preload" href="/assets/js/11.1ec0d6a9.js" as="script"><link rel="prefetch" href="/assets/js/10.08a35d54.js"><link rel="prefetch" href="/assets/js/12.d296e3c5.js"><link rel="prefetch" href="/assets/js/13.edb185f2.js"><link rel="prefetch" href="/assets/js/14.c3f5b097.js"><link rel="prefetch" href="/assets/js/15.12d29b31.js"><link rel="prefetch" href="/assets/js/16.a27bad13.js"><link rel="prefetch" href="/assets/js/17.4e45b22f.js"><link rel="prefetch" href="/assets/js/18.1f174ea9.js"><link rel="prefetch" href="/assets/js/19.98f7a048.js"><link rel="prefetch" href="/assets/js/2.5463c4c1.js"><link rel="prefetch" href="/assets/js/20.6e345427.js"><link rel="prefetch" href="/assets/js/21.df0969a9.js"><link rel="prefetch" href="/assets/js/22.874c846c.js"><link rel="prefetch" href="/assets/js/23.bdf594e8.js"><link rel="prefetch" href="/assets/js/24.201b2c64.js"><link rel="prefetch" href="/assets/js/25.ac5812d2.js"><link rel="prefetch" href="/assets/js/26.2c5bb32a.js"><link rel="prefetch" href="/assets/js/27.e62d850b.js"><link rel="prefetch" href="/assets/js/28.ff4f975d.js"><link rel="prefetch" href="/assets/js/29.9408bee3.js"><link rel="prefetch" href="/assets/js/3.e8f94cd8.js"><link rel="prefetch" href="/assets/js/30.24e008ec.js"><link rel="prefetch" href="/assets/js/31.383112eb.js"><link rel="prefetch" href="/assets/js/32.7571524d.js"><link rel="prefetch" href="/assets/js/33.7edb3943.js"><link rel="prefetch" href="/assets/js/34.89653bb3.js"><link rel="prefetch" href="/assets/js/35.515c62a7.js"><link rel="prefetch" href="/assets/js/36.ab0a0d0a.js"><link rel="prefetch" href="/assets/js/37.775cd637.js"><link rel="prefetch" href="/assets/js/38.3db53492.js"><link rel="prefetch" href="/assets/js/39.188a6347.js"><link rel="prefetch" href="/assets/js/4.9d16c770.js"><link rel="prefetch" href="/assets/js/5.b529b20a.js"><link rel="prefetch" href="/assets/js/6.100ef550.js"><link rel="prefetch" href="/assets/js/7.6256bd33.js"><link rel="prefetch" href="/assets/js/8.278e9f27.js"><link rel="prefetch" href="/assets/js/9.f32132f6.js">
    <link rel="stylesheet" href="/assets/css/0.styles.3208571a.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><section id="global-layout" data-v-4ba9589d><header class="header" data-v-780415cf data-v-4ba9589d><div class="header-navbar" data-v-780415cf><div class="flex-xbc main header-nav" data-v-780415cf><div class="nav-link" data-v-780415cf><a href="/" class="inblock link-logo router-link-active" data-v-780415cf><img data-src="/logo_hci.png" loading="lazy" alt="logo" class="logo-img lazy" data-v-780415cf></a> <br data-v-780415cf> <nav class="link-list" data-v-780415cf><a href="/" class="list-item router-link-active" data-v-780415cf>Home</a><a href="/hci_at_tufts/" class="list-item" data-v-780415cf>HCI At Tufts</a><a href="/people/" class="list-item" data-v-780415cf>People</a><a href="/projects/" class="list-item" data-v-780415cf>Projects</a><a href="/publications/" class="list-item" data-v-780415cf>Publications</a><a href="/code_and_datasets/" class="list-item router-link-active" data-v-780415cf>Code &amp; Datasets</a><a href="/admissions/" class="list-item" data-v-780415cf>Admissions</a><a href="/hci_resources/" class="list-item" data-v-780415cf>HCI Resources</a></nav></div> <!----></div></div> </header> <!----> <section class="page" data-v-4ba9589d data-v-4ba9589d><section class="info" style="background-image:url(/code_and_datasets/fNIRS2MW/fNIRS.png);" data-v-441751fb><article class="main info-content" data-v-22a41398 data-v-441751fb><div class="content-header" data-v-22a41398><h1 class="header-title" data-v-22a41398>The Tufts fNIRS to Mental Workload Dataset</h1></div> <div class="flex-wcc content-tag" data-v-22a41398><div class="inblock tag-list" data-v-22a41398><a href="/category/null/" class="tag-text" data-v-22a41398>
      </a></div> <span class="tag-space" data-v-22a41398>/</span> <div class="inblock tag-list" data-v-22a41398><a href="/tag/BCI/" class="tag-text" data-v-22a41398>BCI
      </a><a href="/tag/Public Dataset/" class="tag-text" data-v-22a41398>Public Dataset
      </a><a href="/tag/fNIRS/" class="tag-text" data-v-22a41398>fNIRS
      </a><a href="/tag/machine learning/" class="tag-text" data-v-22a41398>machine learning
      </a><a href="/tag/time-series classification/" class="tag-text" data-v-22a41398>time-series classification
      </a><a href="/tag/cognitive workload/" class="tag-text" data-v-22a41398>cognitive workload
      </a></div></div> <div class="content content__default" data-v-22a41398><p>Currently, we are waiting for the Tufts IRB to give us permission to release the participants' data. (We are almost there, and we apologize for the inconvenience).</p> <p>If you want to download the data or have any questions, please reach out to Leon (leonwang_at_cs.tufts.edu).</p> <hr> <h2 id="paper-information"><a href="#paper-information" class="header-anchor">#</a> Paper Information</h2> <p>Submitted to NeurIPS 2021 Datasets and Benchmarks Track
Zhe Huang*, Liang Wang*, Giles Blaney, Christopher Slaughter, Devon McKeon, Ziyu Zhou, Alex Olwal, Robert Jacob*, Michael C Hughes*
“The Tufts fNIRS to Mental Workload Dataset: Toward Brain-Computer Interfaces that Generalize”, submitted to NeurIPS 2021 Datasets and Benchmarks Track</p> <p>*Lead authors ZH &amp; LW contributed equally, as did supervisory authors RJ &amp; MCH</p> <p><a href="https://openreview.net/forum?id=QzNHE7QHhut" target="_blank" rel="noopener noreferrer">Paper link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <hr> <h2 id="project-description"><a href="#project-description" class="header-anchor">#</a> Project Description</h2> <p>Functional near-infrared spectroscopy (<strong>fNIRS</strong>) promises a non-intrusive way to measure real-time brain activity and build responsive brain-computer interfaces (<strong>BCIs</strong>). However, in its first decade of research this technology has not yet realized its potential.</p> <ul><li><p>One common <strong>barrier</strong> to effective fNIRS-based BCIs is <strong><em>the lack of available data</em></strong>. Previous work typically collects proprietary datasets from only <code>10</code>-<code>20</code> subjects.</p></li> <li><p>Another <strong>barrier</strong> to progress is the lack of a <strong><em>standardized evaluation protocol</em></strong>. Without standardized protocols, different papers may not follow the very same experimental design, making results incomparable and preventing scientific progress.</p></li> <li><p>The toughest <strong>barrier</strong> of all to developing an accurate mental workload classifier is <strong><em>the high variation in fNIRS data</em></strong>, which makes generalizing to a new subject or session challenging.</p></li></ul> <hr> <p>Our <strong>contributions</strong> are:</p> <ul><li><p>We release <strong><em>a large open-access dataset</em></strong> of <code>68</code> participants. This dataset is the largest known to us by a factor of <code>2.5</code>. Details are in <a href="">Section Dataset</a> below.</p></li> <li><p>We suggest <strong><em>a standardized evaluation practice</em></strong> for assessing method performance on our dataset under three paradigms of training (clear instructions and code are provided in <a href="">Section Code</a> below):</p> <ul><li>subject-specific,</li> <li>generic,</li> <li>generic + fine-tuning.</li></ul></li></ul> <hr> <h2 id="dataset"><a href="#dataset" class="header-anchor">#</a> Dataset</h2> <p>Public dataset release pending imminent approval from IRB.
</p> <hr> <h3 id="license"><a href="#license" class="header-anchor">#</a> License</h3> <p><a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">CC-BY-4.0<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <hr> <h3 id="general-description"><a href="#general-description" class="header-anchor">#</a> General Description</h3> <p>Totally, our large open-access dataset includes <code>68</code> participants. Each subject contributes <code>21.33</code> minutes of fNIRS recordings from a controlled experimental setting with corresponding labels of workload intensity.</p> <hr> <h4 id="task-description"><a href="#task-description" class="header-anchor">#</a> Task Description</h4> <ul><li>Each participant completed <code>16</code> blocks of n-back trails (<em>0→1→2→3→1→2→3→0→2→3→0→1→3→0→1→2</em>).</li> <li>Each block is composed of:
<ul><li><code>40</code> trails ( display for <code>0.5</code> seconds, hidden for <code>1.5</code> seconds).</li></ul></li></ul> <p><em>Details of task sessions is as below:</em> <img alt="task flowchart" data-src="/code_and_datasets/fNIRS2MW/task-flowchart.png" loading="lazy" class="lazy"></p> <hr> <h5 id="task-introduction"><a href="#task-introduction" class="header-anchor">#</a> Task introduction</h5> <p><strong>N-back video introduction</strong>:</p> <p>Before experiment started, the system played a short introductory video, showing an example of a user completing n-back tasks, with voice-over and caption explanations.</p> <p><em>Sample scripts for N-back task video are as below:</em></p> <blockquote><p>Today you will be completing a series of n-back tasks. You will be presented with many numbers, one after another. Some of these numbers will be targets. A number is a target if it is identical to the one shown n steps previously.</p> <p>Here is an example of a 2-back task. In this case, n = 2. The highlighted 8 is a target number, since the number 2 steps back is also 8.</p> <p>Your job will be to press the left arrow key each time you see one of these target numbers. If the number shown is not a target number, you should press the right arrow key.</p></blockquote> <hr> <blockquote><p>Let’s view an example of a 0-back task.</p> <p>At the beginning of each task, you will hear a few beeps which indicate the start of the task.</p> <p>The current type of n-back task can be found at the top of the task window. In this example, n = 0. For the purposes of this demo only, the stack of numbers already shown is included beneath the task window.</p> <p>When n = 0, every number is a target, since every number is identical to the number 0 steps back. That is, every number is the same as itself. For 0-back, the participant presses the left arrow key after each number appears, and never presses the right arrow key.</p></blockquote> <hr> <blockquote><p>Now let’s view an example of a 1-back task.</p> <p>The participant should press the right arrow key for all non-target numbers. When n = 1, a number is only a target if it is identical to the number 1 step back. Once the participant identifies a target number, they should press the left arrow key instead.</p> <p>For example, this 4 is a target number because the previous number was also 4. The participant should press the left arrow key, and then continue pressing the right arrow key until they see the next target number.</p> <p>Here is another target number. The number 1-step back was also 2, so the participant should press the left arrow key.</p> <p>During today’s experiment, the stack of already shown numbers will not be visible to you. It is your job to remember the numbers that were n-steps back so that you are able to identify the correct target numbers.</p></blockquote> <hr> <blockquote><p>There will be 16 rounds in total. During each round, you will complete an n-bask task, where n is either 0, 1, 2, or 3.</p> <p>Before each task, there will be instructions reminding you how to find the target numbers for the current n. During the task, you will be shown 40 numbers. Each time you see a target number, you should press the left arrow key. For all non-target numbers, you should press the right arrow key.</p> <p>After each task, you will complete a survey. Once you have completed the survey, you will be able to rest for 20 seconds.</p> <p>The experiment will take approximately 40 minutes.</p> <p>After the entire experiment is finished, you will complete a short interview about your experience.</p> <p>Please remain seated and do not talk or adjust the headband while completing the experiment. However, if you do feel uncomfortable at any point, let the operator know and they will stop the experiment.</p></blockquote> <hr> <p><strong>Generel introduction</strong>:</p> <p>Before each task, the system displayed a graphic depicting how to identify targets for the current n, with voice-over.</p> <p><em>The graphic and sample scripts for 0-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_0_back.png" width="50%" height="30%"> <blockquote><p>This is a 0 back task. Every number is a target number.</p></blockquote> <hr> <p><em>The graphic and sample scripts for 1-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_1_back.png" width="50%" height="30%"> <blockquote><p>This is a 1 back task. A number is a target if it is identical to the previous number.</p></blockquote> <hr> <p><em>The graphic and sample scripts for 2-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_2_back.png" width="50%" height="30%"> <blockquote><p>This is a 2 back task. A number is a target if it is identical to the number 2 steps back.</p></blockquote> <hr> <p><em>The graphic and sample scripts for 3-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_3_back.png" width="50%" height="30%"> <blockquote><p>This is a 3 back task. A number is a target if it is identical to the number 3 steps back.</p></blockquote> <hr> <h3 id="data-structure"><a href="#data-structure" class="header-anchor">#</a> Data Structure</h3> <p>Our released dataset includes:</p> <ul><li><strong>fNIRS measurements</strong> in <a href="">fNIRS_data</a>;</li> <li><strong>Supplementary data</strong>:
<ul><li><strong>demographic and contextual information</strong> in <a href="">pre-experiment</a>;</li> <li><strong>Cognitive task performance</strong> in <a href="">task_accuracy</a>;</li> <li><strong>experiment log</strong> in <a href="">log</a>;</li> <li><strong>post-experiment interview</strong> in <a href="">interview</a>;</li> <li><strong>subjective workload</strong> in <a href="">nasa-tlx</a>;</li></ul></li></ul> <div class="language- extra-class"><pre class="language-text"><code>***************************************
** fNIRS2MW dataset folder structure **
***************************************
|- qualified_subjects_list.pdf            
|- pre-experiment                              //
|- experiment                                  //
| |- log                                       //
| |- task_accuracy                             //
| |- fNIRS_data                                //
| | |- raw_data                                //
| | |- band_pass_filtered                      //
| | | |- whole_data                            //
| | | |- slide_window_data                     //
| | | | |- size_02sec_10ts_stride_03ts
| | | | |- size_05sec_25ts_stride_03ts
| | | | |- size_10sec_50ts_stride_03ts
| | | | |- size_20sec_100ts_stride_03ts
| | | | |- size_30sec_50ts_stride_03ts
| | | | |- size_40sec_200ts_stride_03ts
|- post-experiment                             //
| |- nasa-tlx                                  //
| |- interview                                 //
| |- * (all other folders)  
</code></pre></div><hr> <h3 id="data-format"><a href="#data-format" class="header-anchor">#</a> Data Format</h3> <p>We introduce and describe the data format of fNIRS data (raw and pre-processed) and supplementary data as below:</p> <hr> <h4 id="fnirs-data"><a href="#fnirs-data" class="header-anchor">#</a> fNIRS Data</h4> <p><code>68</code> participants were recruited, aged <code>18</code> to <code>44</code> years. <code>None</code> of the participants reported neurological, psychiatric, or other brain-related diseases that might affect the result.</p> <hr> <h5 id="raw-data"><a href="#raw-data" class="header-anchor">#</a> raw data</h5> <p>TODO</p> <hr> <h5 id="band-pass-filtered-data"><a href="#band-pass-filtered-data" class="header-anchor">#</a> band pass filtered data</h5> <p>After pre-processing (<strong>Dual-slope</strong> and <strong>band pass filter</strong>), we have <strong><em>features</em></strong>/<strong><em>columns</em></strong> as below:</p> <ul><li><p>label: The label for each row.</p> <ul><li>The values shoule be in the set {0, 1, 2, 3}, representing 0-back/1-back/2-back/3-back tasks respectively.</li> <li>It should be the same for all rows in the same chunk.</li></ul></li> <li><p>chunk (<strong>Only in slide window data</strong>): The chunk number for each chunk.</p> <ul><li>It starts at <code>0</code> and increases by <code>​1</code> for each chunk.</li> <li>It should be the same for all rows in the same chunk.</li></ul></li> <li><p>AB_I_O, AB_I_DO: Intensity of oxy &amp; deoxy from detector AB.</p></li> <li><p>CD_I_O, CD_I_DO: Intensity of oxy &amp; deoxy from detector CD.</p></li> <li><p>AB_PHI_O, AB_PHI_DO: Phase of oxy &amp; deoxy from detector AB.</p></li> <li><p>CD_PHI_O, CD_PHI_DO: Phase of oxy &amp; deoxy from detector CD.</p></li></ul> <p><img alt="pre process" data-src="/code_and_datasets/fNIRS2MW/pre-processing.png" loading="lazy" class="lazy"></p> <h6 id="whole-data"><a href="#whole-data" class="header-anchor">#</a> whole data</h6> <p>Each subject's .csv file includes continuous data of <code>16</code> tasks (exclude data during self-evaluation (<a href="">nasa-tlx</a>) and rest period).</p> <p><em>Screenshot of deidentified data sample is as below:</em> <img alt="whole data" data-src="/code_and_datasets/fNIRS2MW/bpf_whole_data.png" loading="lazy" class="lazy"></p> <hr> <h6 id="slide-window-data"><a href="#slide-window-data" class="header-anchor">#</a> slide window data</h6> <p>We offer pre processed data in :</p> <ul><li>Window size: <code>10</code>/<code>25</code>/<code>50</code>/<code>100</code>/<code>150</code>/<code>200</code> timestamps (<code>2</code>/<code>5</code>/<code>10</code>/<code>20</code>/<code>30</code>/<code>40</code> seconds)</li> <li>Window stride: <code>3</code> timestamps (rough <code>0.6</code> second)</li></ul> <p><em>Screenshot of deidentified data sample is as below:</em> <img alt="slide window data" data-src="/code_and_datasets/fNIRS2MW/bpf_slide_window_data.png" loading="lazy" class="lazy"></p> <hr> <h4 id="supplementary-data"><a href="#supplementary-data" class="header-anchor">#</a> Supplementary Data</h4> <p>To ensure quality and consistency, we used several criteria to identify which subjects' data are suitable for classifier evaluation.</p> <hr> <h5 id="demographic-and-contextual-information"><a href="#demographic-and-contextual-information" class="header-anchor">#</a> demographic and contextual information</h5> <p>Demographic and contextual information is recorded before the experiment.</p> <p><em>Please see details in the screenshot of fake data sample as below:</em> <img alt="pre-experiment" data-src="/code_and_datasets/fNIRS2MW/pre-experiment.png" loading="lazy" class="lazy"></p> <hr> <h5 id="task-accuracy"><a href="#task-accuracy" class="header-anchor">#</a> task_accuracy</h5> <p>We measured the subject's performance at the n-back task based on the accuracy of the subject's response for each digit.</p> <p>the accuracy of each n-back task was recorded.</p> <p><em>Please see details in the screenshot of fake data sample as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/task_accuracy.png" width="30%" height="30%"> <hr> <h5 id="nasa-tlx"><a href="#nasa-tlx" class="header-anchor">#</a> nasa-tlx</h5> <p>This is a good way to evaluate the <strong>perceived/subjective</strong> mental workload.</p> <p>Total <code>16</code> &quot;serial_feedback&quot; .csv files for <code>16</code> n-back tasks:</p> <ul><li><code>12</code> files start with &quot;train_1_1_&quot; (<code>1</code>-<code>12</code>) match the first <code>1</code>-<code>12</code> tasks,</li> <li><code>4</code> files start with &quot;test_1_2_&quot; (<code>1</code>-<code>4</code>) match the last <code>4</code> tasks.</li> <li>Useful features include:
<ul><li>movement: subject report if the headband is moved or not,</li> <li>uncomfortable: subject report if feeling unfortable during the experiment or not,</li> <li>mental: mental workload after the task from low (<code>0</code>) to high (<code>100</code>)</li> <li>performance: performance after the task from low (<code>0</code>) to high (<code>100</code>)</li> <li>effort: effort needed for the task from low (<code>0</code>) to high (<code>100</code>)</li> <li>frustration: frustration felt during the task from low (<code>0</code>) to high (<code>100</code>)</li></ul></li></ul> <p><em>Please see details in the screenshot of deidentified data sample as below:</em> <img alt="nasa-tlx" data-src="/code_and_datasets/fNIRS2MW/nasa-tlx.png" loading="lazy" class="lazy"></p> <hr> <h5 id="log"><a href="#log" class="header-anchor">#</a> log</h5> <p>Hair blocking, light leaking, fNIRS instrument settings and other issues during the experiment were recorded.</p> <hr> <h5 id="interview"><a href="#interview" class="header-anchor">#</a> interview</h5> <p>Post-experiment interviews were converted from audio to text (pdf version) by the operator.</p> <p>The original audios were <strong>destroyed</strong> immediately following the IRB protocol.</p> <hr> <h2 id="code"><a href="#code" class="header-anchor">#</a> Code</h2></div> <div class="content-time" data-v-22a41398><time datetime="Feb 19, 2019" class="time-text" data-v-22a41398>Create Time: Feb 19, 2019
    </time> <time datetime="Aug 27, 2021" class="time-text" data-v-22a41398>Last Updated: Aug 27, 2021
    </time></div></article> <section class="flex-xb main info-nav" data-v-64012905 data-v-441751fb><!----> <a href="/code_and_datasets/UIST2021.html" class="flex-xb nav-item" data-v-64012905><div class="flex-xcc item-img" data-v-64012905><img data-src="/code_and_datasets/three_phases.png" loading="lazy" alt="Taming fNIRS-based BCI Input for Better Calibration and Broader Use" class="img lazy" data-v-64012905></div> <article class="flex-ysc item-content" data-v-64012905><h2 class="content-title" data-v-64012905>Taming fNIRS-based BCI Input for Better Calibration and Broader Use</h2> <div class="content" data-v-64012905></div></article></a></section> <!----></section></section> <div data-v-55aa431a data-v-4ba9589d><div class="my-box" data-v-55aa431a></div> <footer class="footer" data-v-55aa431a><nav class="link-list" data-v-55aa431a><a href="/code_and_datasets/fNIRS2MW.html" aria-current="page" class="list-item router-link-exact-active router-link-active" data-v-55aa431a>HCI Lab, Department of Computer Science 196 Boston Ave., Tufts University, Medford, MA, 02155</a></nav> <a href="/" class="copyright router-link-active" data-v-55aa431a>Copyright  ©  Tufts University School of Engineering. All Rights Reserved.
      </a></footer></div></section><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.c05da18d.js" defer></script><script src="/assets/js/11.1ec0d6a9.js" defer></script>
  </body>
</html>
