<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The Tufts fNIRS to Mental Workload Dataset | Tufts HCI Lab</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="Tufts Human-Computer Interaction Laboratory">
    
    <link rel="preload" href="/assets/css/0.styles.3208571a.css" as="style"><link rel="preload" href="/assets/js/app.7aa76c97.js" as="script"><link rel="preload" href="/assets/js/10.d3851ead.js" as="script"><link rel="prefetch" href="/assets/js/11.23de9b2d.js"><link rel="prefetch" href="/assets/js/12.46097dd5.js"><link rel="prefetch" href="/assets/js/13.3719f475.js"><link rel="prefetch" href="/assets/js/14.85ed9838.js"><link rel="prefetch" href="/assets/js/15.12b7bd5c.js"><link rel="prefetch" href="/assets/js/16.0bccb51c.js"><link rel="prefetch" href="/assets/js/17.779b5fe6.js"><link rel="prefetch" href="/assets/js/18.a7ac8530.js"><link rel="prefetch" href="/assets/js/19.33e9e599.js"><link rel="prefetch" href="/assets/js/2.5463c4c1.js"><link rel="prefetch" href="/assets/js/20.8f8302ca.js"><link rel="prefetch" href="/assets/js/21.270e6e1d.js"><link rel="prefetch" href="/assets/js/22.e305c3fd.js"><link rel="prefetch" href="/assets/js/23.7b5adc1d.js"><link rel="prefetch" href="/assets/js/24.c7356405.js"><link rel="prefetch" href="/assets/js/25.05fcd92d.js"><link rel="prefetch" href="/assets/js/26.a8534a61.js"><link rel="prefetch" href="/assets/js/27.933e5ae4.js"><link rel="prefetch" href="/assets/js/28.9ddfdf6a.js"><link rel="prefetch" href="/assets/js/29.4682fc66.js"><link rel="prefetch" href="/assets/js/3.e8f94cd8.js"><link rel="prefetch" href="/assets/js/30.94a87879.js"><link rel="prefetch" href="/assets/js/31.11fadcfc.js"><link rel="prefetch" href="/assets/js/32.41699f30.js"><link rel="prefetch" href="/assets/js/33.778ce70b.js"><link rel="prefetch" href="/assets/js/34.adcf53d9.js"><link rel="prefetch" href="/assets/js/35.50234c1c.js"><link rel="prefetch" href="/assets/js/36.ab0a0d0a.js"><link rel="prefetch" href="/assets/js/37.775cd637.js"><link rel="prefetch" href="/assets/js/38.f7b75fc9.js"><link rel="prefetch" href="/assets/js/39.3c58ec0a.js"><link rel="prefetch" href="/assets/js/4.9d16c770.js"><link rel="prefetch" href="/assets/js/5.b529b20a.js"><link rel="prefetch" href="/assets/js/6.100ef550.js"><link rel="prefetch" href="/assets/js/7.89385fd8.js"><link rel="prefetch" href="/assets/js/8.5a0b0019.js"><link rel="prefetch" href="/assets/js/9.ad461b56.js">
    <link rel="stylesheet" href="/assets/css/0.styles.3208571a.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><section id="global-layout" data-v-4ba9589d><header class="header" data-v-780415cf data-v-4ba9589d><div class="header-navbar" data-v-780415cf><div class="flex-xbc main header-nav" data-v-780415cf><div class="nav-link" data-v-780415cf><a href="/" class="inblock link-logo router-link-active" data-v-780415cf><img data-src="/logo_hci.png" loading="lazy" alt="logo" class="logo-img lazy" data-v-780415cf></a> <br data-v-780415cf> <nav class="link-list" data-v-780415cf><a href="/" class="list-item router-link-active" data-v-780415cf>Home</a><a href="/hci_at_tufts/" class="list-item" data-v-780415cf>HCI At Tufts</a><a href="/people/" class="list-item" data-v-780415cf>People</a><a href="/projects/" class="list-item" data-v-780415cf>Projects</a><a href="/publications/" class="list-item" data-v-780415cf>Publications</a><a href="/code_and_datasets/" class="list-item router-link-active" data-v-780415cf>Code &amp; Datasets</a><a href="/admissions/" class="list-item" data-v-780415cf>Admissions</a><a href="/hci_resources/" class="list-item" data-v-780415cf>HCI Resources</a></nav></div> <!----></div></div> </header> <!----> <section class="page" data-v-4ba9589d data-v-4ba9589d><section class="info" style="background-image:url(/code_and_datasets/fNIRS.png);" data-v-441751fb><article class="main info-content" data-v-22a41398 data-v-441751fb><div class="content-header" data-v-22a41398><h1 class="header-title" data-v-22a41398>The Tufts fNIRS to Mental Workload Dataset</h1></div> <div class="flex-wcc content-tag" data-v-22a41398><div class="inblock tag-list" data-v-22a41398><a href="/category/null/" class="tag-text" data-v-22a41398>
      </a></div> <span class="tag-space" data-v-22a41398>/</span> <div class="inblock tag-list" data-v-22a41398><a href="/tag/BCI/" class="tag-text" data-v-22a41398>BCI
      </a><a href="/tag/Public Dataset/" class="tag-text" data-v-22a41398>Public Dataset
      </a><a href="/tag/fNIRS/" class="tag-text" data-v-22a41398>fNIRS
      </a><a href="/tag/machine learning/" class="tag-text" data-v-22a41398>machine learning
      </a></div></div> <div class="content content__default" data-v-22a41398><p>Currently, we are waiting for the Tufts IRB to give us permission to release the participants' data. (We are almost there, and we apologize for the inconvenience).</p> <p>If you want to download the data or have any questions, please reach out to Leon (leonwang_at_cs.tufts.edu).</p> <h2 id="paper-information"><a href="#paper-information" class="header-anchor">#</a> Paper Information</h2> <p>Zhe Huang*, Liang Wang*, Giles Blaney, Christopher Slaughter, Devon McKeon, Ziyu Zhou, Alex Olwal, Robert Jacob*, Michael C Hughes*
“The Tufts fNIRS to Mental Workload Dataset: Toward Brain-Computer Interfaces that Generalize” NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</p> <p>*Lead authors ZH &amp; LW contributed equally, as did supervisory authors RJ &amp; MCH</p> <h2 id="description"><a href="#description" class="header-anchor">#</a> Description</h2> <p>Functional near-infrared spectroscopy (fNIRS) promises a non-intrusive way to measure real-time brain activity and build responsive brain-computer interfaces. However, in its first decade of research this technology has not yet realized its potential. A primary barrier to success has been that observed fNIRS signals vary significantly across human users. Building models that generalize well to never-before-seen users has been difficult; a large amount of subject-specific data has been needed to train effective models. To help overcome this barrier, we introduce the largest open-access dataset of its kind, containing multivariate fNIRS recordings from over 60 participants, each with labeled segments indicating four possible mental workload intensity levels. Labels were collected via a controlled setting in which subjects performed standard n-back tasks to induce desired working memory levels. We further define an evaluation protocol which allows future users to report comparable numbers and fairly assess generalization potential while avoiding any overlap or leakage between train and test data. Using this dataset and standardized protocol, we show how deep learning models pre-trained using abundant fNIRS data from other participants can be effectively fine-tuned for new subjects, reaching accuracies similar to data-hungry subject-specific models while using a fraction of the data. We further show how performance improves as the size of the available dataset grows, while also analyzing error rates across key subpopulations to explore equity concerns. We share our open-access dataset and open-source code as a step toward advancing brain computer interfaces that work for many users.</p> <h2 id="code"><a href="#code" class="header-anchor">#</a> Code</h2> <p><a href="https://github.com/lwang89/code_for_UIST.git" target="_blank" rel="noopener noreferrer">Here is the link to our code Github repo<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="dataset"><a href="#dataset" class="header-anchor">#</a> Dataset</h2> <p>Public dataset release pending imminent approval from IRB.
</p> <h3 id="data-description"><a href="#data-description" class="header-anchor">#</a> Data Description</h3> <h3 id="data-structure"><a href="#data-structure" class="header-anchor">#</a> Data Structure</h3> <h3 id="data-format"><a href="#data-format" class="header-anchor">#</a> Data Format</h3> <h4 id="pre-experiment"><a href="#pre-experiment" class="header-anchor">#</a> Pre-experiment</h4> <p>It includes the non sensitive personal data we collect before the experiment.</p> <h4 id="experiment-data"><a href="#experiment-data" class="header-anchor">#</a> Experiment Data</h4> <p>All fNIRS data store here, along with the n-back tasks accuracy and experiment log.</p> <ol><li>raw data;</li> <li>band-pass-filtered;
2.a. bpf_raw_data;
2.b. bpf_filtered_slide_window_data;</li></ol> <h4 id="supplementary-data"><a href="#supplementary-data" class="header-anchor">#</a> Supplementary Data</h4> <h5 id="demographic-and-contextual-information"><a href="#demographic-and-contextual-information" class="header-anchor">#</a> Demographic and contextual information</h5> <h5 id="subjective-workload"><a href="#subjective-workload" class="header-anchor">#</a> subjective workload</h5> <h5 id="post-experiment-interview"><a href="#post-experiment-interview" class="header-anchor">#</a> post-experiment interview</h5> <h2 id="paper-link-and-please-cite"><a href="#paper-link-and-please-cite" class="header-anchor">#</a> Paper link and Please Cite</h2> <p><a href="https://openreview.net/forum?id=QzNHE7QHhut" target="_blank" rel="noopener noreferrer">Paper link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <div class="content-time" data-v-22a41398><time datetime="Feb 19, 2019" class="time-text" data-v-22a41398>Create Time: Feb 19, 2019
    </time> <time datetime="Aug 25, 2021" class="time-text" data-v-22a41398>Last Updated: Aug 25, 2021
    </time></div></article> <section class="flex-xb main info-nav" data-v-64012905 data-v-441751fb><!----> <a href="/code_and_datasets/UIST2021.html" class="flex-xb nav-item" data-v-64012905><div class="flex-xcc item-img" data-v-64012905><img data-src="/code_and_datasets/three_phases.png" loading="lazy" alt="Taming fNIRS-based BCI Input for Better Calibration and Broader Use" class="img lazy" data-v-64012905></div> <article class="flex-ysc item-content" data-v-64012905><h2 class="content-title" data-v-64012905>Taming fNIRS-based BCI Input for Better Calibration and Broader Use</h2> <div class="content" data-v-64012905></div></article></a></section> <!----></section></section> <div data-v-55aa431a data-v-4ba9589d><div class="my-box" data-v-55aa431a></div> <footer class="footer" data-v-55aa431a><nav class="link-list" data-v-55aa431a><a href="/code_and_datasets/NeurIPS.html" aria-current="page" class="list-item router-link-exact-active router-link-active" data-v-55aa431a>HCI Lab, Department of Computer Science 196 Boston Ave., Tufts University, Medford, MA, 02155</a></nav> <a href="/" class="copyright router-link-active" data-v-55aa431a>Copyright  ©  Tufts University School of Engineering. All Rights Reserved.
      </a></footer></div></section><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.7aa76c97.js" defer></script><script src="/assets/js/10.d3851ead.js" defer></script>
  </body>
</html>
